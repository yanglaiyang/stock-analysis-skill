#!/usr/bin/env python3
"""
è‚¡ç¥¨ç®€åŒ–åˆ†ææ³• - ä¸“ä¸šè‚¡ç¥¨åˆ†æå·¥å…· (Subagent æ¶æ„ç‰ˆ)
Stock Simplified Analysis Method (Subagent Architecture)

ä½¿ç”¨ Google Gemini AI + Tushare MCP è¿›è¡Œç³»ç»ŸåŒ–çš„è‚¡ç¥¨åˆ†æã€‚
åŸºäº 7 æ­¥ Subagent æ¶æ„ï¼Œæ¯ä¸ªæ­¥éª¤ç”±ä¸“é—¨çš„ AI æ™ºèƒ½ä½“è´Ÿè´£ã€‚
"""

import os
import sys
import argparse
import time
from typing import Optional, List
import urllib.parse
import requests
from bs4 import BeautifulSoup

# å¯¼å…¥æ–°çš„ Subagent æ¶æ„
from subagents import SubagentOrchestrator

# ç³»ç»Ÿæç¤ºè¯ï¼ˆç”¨äºæµ‹è¯•ä¸ä¸€è‡´æ€§æ ¡éªŒï¼‰
SYSTEM_PROMPT = """
æ­¥éª¤ä¸€ï¼šä¸šåŠ¡å¢é•¿å‘¨æœŸåˆ†æ
æ­¥éª¤äºŒï¼šä¸šåŠ¡åˆ†æ
æ­¥éª¤ä¸‰ï¼šæŠ¤åŸæ²³åˆ†æ
æ­¥éª¤å››ï¼šé•¿æœŸæ½œåŠ›åˆ†æ
æ­¥éª¤äº”ï¼šå…³é”®æŒ‡æ ‡åˆ†æ
æ­¥éª¤å…­ï¼šé£é™©åˆ†æ
æ­¥éª¤ä¸ƒï¼šä¼°å€¼åˆ†æ

æŠ¥å‘Šç»“æ„ï¼š
- æ‰§è¡Œæ‘˜è¦
- ä¸šåŠ¡é˜¶æ®µåˆ†æ
- ä¸šåŠ¡æ¨¡å¼åˆ†æ
- æŠ¤åŸæ²³åˆ†æ
- é•¿æœŸå¢é•¿æ½œåŠ›åˆ†æ
- å…³é”®æŒ‡æ ‡å¥åº·æ£€æŸ¥
- æ‰§è¡Œé£é™©è¯„ä¼°
- ä¼°å€¼æ¡†æ¶åˆ†æ
""".strip()


class StockAnalyzer:
    """è‚¡ç¥¨åˆ†æå™¨ä¸»ç±» (Subagent æ¶æ„å°è£…)"""

    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        Args:
            api_key: Gemini APIå¯†é’¥
        """
        self.api_key = api_key or os.getenv('GEMINI_API_KEY') or os.getenv('GOOGLE_API_KEY')
        if not self.api_key:
            raise ValueError("è¯·è®¾ç½® GEMINI_API_KEY æˆ– GOOGLE_API_KEY ç¯å¢ƒå˜é‡")

        # åˆå§‹åŒ– Orchestrator
        self.orchestrator = SubagentOrchestrator(self.api_key)
        print("âœ… AIè´¢åŠ¡åˆ†æå¸ˆå·²åˆå§‹åŒ–ï¼ˆSubagentæ¶æ„ + Tushare MCPï¼‰ï¼Œå‡†å¤‡å°±ç»ªã€‚")

    def get_webpage_title(self, url: str) -> str:
        """è·å–ç½‘é¡µæˆ–æ–‡ä»¶æ ‡é¢˜"""
        def _extract_filename(raw_url: str) -> str:
            decoded_url = urllib.parse.unquote(raw_url)
            clean_url = decoded_url.split('?', 1)[0]
            filename = clean_url.split('/')[-1]
            return filename.strip() if filename.strip() else "å¤–éƒ¨å‚è€ƒæ–‡æ¡£"

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        try:
            response = requests.get(url, headers=headers, timeout=5, stream=True)
            if 'text/html' in response.headers.get('Content-Type', '').lower():
                response.encoding = response.apparent_encoding
                soup = BeautifulSoup(response.content, 'html.parser')
                if soup.title and soup.title.string:
                    return soup.title.string.strip()
            return _extract_filename(url)
        except Exception as e:
            print(f"âš ï¸ æ— æ³•æŠ“å–æ ‡é¢˜ ({url}): {e}")
            return _extract_filename(url)
        return "å¤–éƒ¨å‚è€ƒæ–‡æ¡£"

    def format_links(self, links: List[str]) -> str:
        """æ ¼å¼åŒ–é“¾æ¥åˆ—è¡¨ä¸ºMarkdown"""
        formatted_links = []
        for link in links:
            title = self.get_webpage_title(link)
            formatted_links.append(f"- [{title}]({link})")
        return "\n".join(formatted_links)

    def analyze(
        self,
        company: str,
        links: Optional[List[str]] = None,
        file_paths: Optional[List[str]] = None,
        output_file: Optional[str] = None,
        max_retries: int = 5
    ) -> str:
        """æ‰§è¡Œè‚¡ç¥¨åˆ†æ"""
        
        # 1. å¤„ç†è¾“å…¥æ•°æ®
        pdf_content = ""
        if file_paths:
            print(f"ğŸ“„ å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶: {len(file_paths)} ä¸ª")
            contents = []
            for fp in file_paths:
                if os.path.exists(fp):
                    try:
                        # å°è¯•ä½¿ç”¨ pdf_generator ä¸­çš„å·¥å…·æˆ–ç®€å•è¯»å–
                        with open(fp, 'r', encoding='utf-8', errors='ignore') as f:
                             contents.append(f.read())
                    except Exception as e:
                        print(f"âŒ è¯»å–æ–‡ä»¶ {fp} å¤±è´¥: {e}")
            pdf_content = "\n".join(contents)

        formatted_links = ""
        if links:
            formatted_links = self.format_links(links)
            if formatted_links:
                pdf_content += f"\n\nå‚è€ƒé“¾æ¥:\n{formatted_links}"

        # 2. è§£æè‚¡ç¥¨ä»£ç 
        ts_code = None
        if ',' in company:
            parts = company.split(',')
            if len(parts) >= 2:
                ts_code = parts[1].strip()

        # 3. è¿è¡Œ Subagent Orchestrator
        try:
            print(f"\nï¿½ å¯åŠ¨ Subagent åˆ†ææµç¨‹: {company}")
            if ts_code:
                print(f"ğŸ¯ è‚¡ç¥¨ä»£ç : {ts_code}")
            
            report = self.orchestrator.run(
                company=company,
                ts_code=ts_code,
                pdf_content=pdf_content
            )
            
            # 4. ä¿å­˜æŠ¥å‘Š (å¦‚æœ run æ–¹æ³•è¿”å›çš„æ˜¯æ–‡ä»¶è·¯å¾„)
            if output_file and report:
                # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯ HTML æ–‡ä»¶
                if report.endswith('.html'):
                    # å¦‚æœç”¨æˆ·æŒ‡å®šäº†è¾“å‡ºè·¯å¾„ï¼Œä¸”ä¸é»˜è®¤ç”Ÿæˆçš„ä¸ä¸€è‡´ï¼Œåˆ™ç§»åŠ¨æˆ–é‡å‘½å
                    if output_file != report:
                        import shutil
                        shutil.move(report, output_file)
                        print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_file}")
                        return output_file
                    else:
                        print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜è‡³: {report}")
                        return report
                else:
                    # ä¼ ç»Ÿçš„ Markdown å†™å…¥
                    with open(output_file, 'w', encoding='utf-8') as f:
                        f.write(report)
                    print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_file}")
            
            return report

        except Exception as e:
            print(f"\nâŒ åˆ†æè¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return ""


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description='è‚¡ç¥¨ç®€åŒ–åˆ†ææ³• - ä¸“ä¸šè‚¡ç¥¨åˆ†æå·¥å…· (Subagentç‰ˆ)',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument('-c', '--company', required=True,
                        help='å…¬å¸åç§°å’Œä»£ç ï¼Œæ ¼å¼: "å…¬å¸å, ä»£ç "')
    parser.add_argument('-l', '--links',
                        help='å‚è€ƒé“¾æ¥ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰')
    parser.add_argument('-f', '--files', nargs='+',
                        help='ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒå¤šä¸ªï¼‰')
    parser.add_argument('-o', '--output',
                        help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼ˆMarkdownæ ¼å¼ï¼‰')
    parser.add_argument('-k', '--api-key',
                        help='Gemini APIå¯†é’¥')
    parser.add_argument('--retries', type=int, default=5,
                        help='æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤: 5ï¼‰')

    args = parser.parse_args()

    # åˆå§‹åŒ–å¹¶è¿è¡Œ
    try:
        analyzer = StockAnalyzer(api_key=args.api_key)
        report = analyzer.analyze(
            company=args.company,
            links=[l.strip() for l in args.links.split(',')] if args.links else None,
            file_paths=args.files,
            output_file=args.output,
            max_retries=args.retries
        )
        
        if report:
            print("\nåˆ†æå®Œæˆï¼")
        else:
            sys.exit(1)

    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
